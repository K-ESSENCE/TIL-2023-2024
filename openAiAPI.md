
### 생성형 AI? 

한 문자 생성 ? => 단어 조합 생성 
문자를 조각내는 기준? =>  자연어들의 조각의 기준이 컴퓨터한테는 ? => 그림이나 영상 같은 건 어떻게 조각내는가? => 통계적으로 분석해서 쪼개자 => 토큰 아이디어를 만드는 방법 

컴퓨터가 생성하고 싶은걸 기준으로 쪼갠다.


AI => 컴퓨터가 못할걸 한다는 걸 한다는 측면에서 대단하다는 느낌 
뭔갈 생성한다기보다 그냥 '선택' 하고 고르는걸 잘하는 게 기존의 AI 였으나 이젠 생성한다는 측면

컴퓨팅 리소스 => 돈 

### LLM의 입력과 출력 

Input 
토큰으로 이루어진 배열 
토큰 : 자연어 쪼갈낸거 

Output 
토큰 배열 다음에 가장 있을 법한 토큰 

입력 / 쪼개기 / 다음에 올거 고르기 / 고르는거 반복/ 끊기 /출력 

다음에 올 토큰 고르기 => 토큰 배열 / 임베딩 벡터 배열/ 복잡한 연산 / context 벡터/ 다음 토큰의 분포 / 다음토큰

### LLM을 학습한다는건 ? 

1. 입력언어를 하나의 Context 벡터로 치환하는 방법을 배우는것 => 토큰 별로 적절한 임베딩 벡터 매핑 , 가중치 업데이트 
2. Context벡터를 가지고 입력언어 다음에 올 토큰의 분포확률을 계산하는 방법을 배우는것

GPT? -> generative pre-trained transformer 
사전학습된 트랜스포머 


Fine-Tuning & RLHF => LLM이 사람의 말을 잘 따르도록 학습 

open Ai API 에는 Prompting이 빠져있기때문에 이걸 잘하기위한 작업도 필요하다. 


